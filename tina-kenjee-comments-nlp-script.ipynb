{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conceptual-times",
   "metadata": {
    "_cell_guid": "75fe1c7a-7f3a-4c74-8a75-1fdfff6d0a13",
    "_uuid": "347c87fd-7bb5-4687-a1af-effffc30108a",
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:21.885264Z",
     "iopub.status.busy": "2021-05-17T12:45:21.884098Z",
     "iopub.status.idle": "2021-05-17T12:45:21.892592Z",
     "shell.execute_reply": "2021-05-17T12:45:21.893132Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022984,
     "end_time": "2021-05-17T12:45:21.893482",
     "exception": false,
     "start_time": "2021-05-17T12:45:21.870498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "large-familiar",
   "metadata": {
    "_cell_guid": "57a49c62-5d8b-45d9-a685-374e273342c7",
    "_uuid": "b7f70aba-d7f5-499b-8026-80c526b726fb",
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:21.914795Z",
     "iopub.status.busy": "2021-05-17T12:45:21.914151Z",
     "iopub.status.idle": "2021-05-17T12:45:21.917904Z",
     "shell.execute_reply": "2021-05-17T12:45:21.918435Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015791,
     "end_time": "2021-05-17T12:45:21.918618",
     "exception": false,
     "start_time": "2021-05-17T12:45:21.902827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_func():\n",
    "    print(\"it works!! :o)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4841df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_question(row):\n",
    "    return row.find('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66db26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_words = pd.read_csv('list_swear.txt', delimiter = \"\\n\")\n",
    "swear_words_set = set(swear_words['4r5e'])\n",
    "swear_words_set.add('4r5e')\n",
    "\n",
    "def swear_o_meter_detection(row):\n",
    "    swear_o_meter=0\n",
    "#     print(row)\n",
    "    text = row.split()\n",
    "    for i in text:\n",
    "        if i in swear_words_set:\n",
    "            swear_o_meter += 1\n",
    "    \n",
    "    return swear_o_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff98ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_text_entities(data):\n",
    "    \n",
    "    temp  = concat_all_text(data)\n",
    "    doc = nlp(temp)\n",
    "    entities = []\n",
    "    \n",
    "#     ent_pd = []\n",
    "    \n",
    "#     for ent in doc.ents:\n",
    "#         ent_pd.append[ent.text, ent.start_char, ent.end_char, ent.label_]\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    \n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entities.append([ent.text, ent.start_char, ent.end_char, ent.label_])\n",
    "#         entities.extend([{ent}, {ent.label_}, {spacy.explain(ent.label_)}])\n",
    "#         entities.extend(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')\n",
    "#         print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constant-neighborhood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:21.943431Z",
     "iopub.status.busy": "2021-05-17T12:45:21.942753Z",
     "iopub.status.idle": "2021-05-17T12:45:21.944682Z",
     "shell.execute_reply": "2021-05-17T12:45:21.945121Z"
    },
    "papermill": {
     "duration": 0.017479,
     "end_time": "2021-05-17T12:45:21.945313",
     "exception": false,
     "start_time": "2021-05-17T12:45:21.927834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stop_words(type=\"stopwordslib\"):\n",
    "    more_stopwords = set(['t','dtypes','n','dtype','v','wo','s','object'])\n",
    "    if(type==\"stopwordslib\"):      \n",
    "        from nltk.corpus import stopwords\n",
    "        my_stop = stopwords.words('english')\n",
    "#         my_stop = my_stop.union( more_stopwords)\n",
    "    elif(type==\"from_wordcloud\"):\n",
    "        my_stop =  set(STOPWORDS)\n",
    "        my_stop = my_stop.union( more_stopwords)\n",
    "    \n",
    "    return my_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-malpractice",
   "metadata": {
    "_cell_guid": "186c279b-0c32-46a7-8d0a-1605786935fa",
    "_uuid": "ebaaeec0-243b-473b-b098-40a38cf73dcc",
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:21.966757Z",
     "iopub.status.busy": "2021-05-17T12:45:21.966081Z",
     "iopub.status.idle": "2021-05-17T12:45:40.454247Z",
     "shell.execute_reply": "2021-05-17T12:45:40.453499Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 18.499893,
     "end_time": "2021-05-17T12:45:40.454442",
     "exception": false,
     "start_time": "2021-05-17T12:45:21.954549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\users\\pascal\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\pascal\\appdata\\roaming\\python\\python38\\site-packages (from TextBlob) (3.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\pascal\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\pascal\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pascal\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pascal\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "### Import Statements ###\n",
    "\n",
    "# Graphs section\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Imports for wordcloud\n",
    "# from subprocess import check_output\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Python spellchecker\n",
    "# !pip install pyspellchecker\n",
    "# from spellchecker import SpellChecker\n",
    "# import string\n",
    "# Textblob -> Taken from tina's notebook mentioned above :o)\n",
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "# For regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "duplicate-render",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.495173Z",
     "iopub.status.busy": "2021-05-17T12:45:40.494520Z",
     "iopub.status.idle": "2021-05-17T12:45:40.499658Z",
     "shell.execute_reply": "2021-05-17T12:45:40.499121Z"
    },
    "papermill": {
     "duration": 0.027433,
     "end_time": "2021-05-17T12:45:40.499825",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.472392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Regex 'Mask' to filter out garbage characters and stuff\n",
    "regex = r\"[^0-9a-zA-Z\\']+\"\n",
    "\n",
    "def clean_lines(row):\n",
    "    subst = \" \"\n",
    "    row = re.sub(regex,subst,row,0)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prepared-problem",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.540678Z",
     "iopub.status.busy": "2021-05-17T12:45:40.539389Z",
     "iopub.status.idle": "2021-05-17T12:45:40.543250Z",
     "shell.execute_reply": "2021-05-17T12:45:40.543775Z"
    },
    "papermill": {
     "duration": 0.027009,
     "end_time": "2021-05-17T12:45:40.543975",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.516966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to concat all text from a column into a single string\n",
    "def concat_all_text(data_col):\n",
    "    text = data_col.str.cat(sep=' ')\n",
    "\n",
    "    # removing URLs and '&amp' substrings using regex\n",
    "    import re\n",
    "    url_reg  = r'[a-z]*[:.]+\\S+'\n",
    "    text   = re.sub(url_reg, '', text)\n",
    "    noise_reg = r'\\&amp'\n",
    "    text   = re.sub(noise_reg, '', text)\n",
    "    regex = r\"[^0-9a-zA-Z\\']+\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex,subst,text,0)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "descending-necklace",
   "metadata": {
    "_cell_guid": "2abe9923-8f47-4099-ad6a-2dad497c77f9",
    "_uuid": "87cd37e9-6da5-4ed5-9d0c-42f309d136c6",
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.588746Z",
     "iopub.status.busy": "2021-05-17T12:45:40.587776Z",
     "iopub.status.idle": "2021-05-17T12:45:40.590589Z",
     "shell.execute_reply": "2021-05-17T12:45:40.591027Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.02938,
     "end_time": "2021-05-17T12:45:40.591198",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.561818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thanks to thte following kaggler for this notebook! ->  https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove/notebook#Data-Cleaning\n",
    "# Section containing various \"text cleaning\" functions -> WIP!\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def full_clean(text):\n",
    "    text = remove_punct(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_URL(text)\n",
    "    text = remove_emoji(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indoor-collector",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.636444Z",
     "iopub.status.busy": "2021-05-17T12:45:40.635498Z",
     "iopub.status.idle": "2021-05-17T12:45:40.638818Z",
     "shell.execute_reply": "2021-05-17T12:45:40.638299Z"
    },
    "papermill": {
     "duration": 0.030486,
     "end_time": "2021-05-17T12:45:40.638962",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.608476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline to process a full column from a dataset\n",
    "def processRow(row):\n",
    "    \n",
    "    # Simple porcessing stuff (lowercasing, removing emoji,URL etc ...)\n",
    "    temp = row.lower()\n",
    "    temp = remove_emoji(temp)\n",
    "    temp = remove_URL(temp)\n",
    "    temp = remove_html(temp)\n",
    "    temp = re.sub(r'[^\\w\\s]',\" \",temp)\n",
    "    \n",
    "    # remove numbers\n",
    "    temp = \"\".join([i for i in temp if not i.isdigit()])\n",
    "    \n",
    "    # Expand contraction ('im' -> 'i am' )\n",
    "    temp = expand_contractions(temp)\n",
    "    # Lemmatize & remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    my_stop = set(stopwords.words('english'))\n",
    "    temp = \" \".join([Word(word).lemmatize() for word in temp.split() if word not in my_stop if len(word)>1])\n",
    "    \n",
    "    # Return finalized row\n",
    "    return temp\n",
    "\n",
    "# Function to get the frequencies \n",
    "def get_fequencies(doc_column):\n",
    "    doc_column = concat_all_text(doc_column)\n",
    "    tokens = nltk.word_tokenize(doc_column)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "    import operator\n",
    "    sorted_x = sorted(frequency_dist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    sorted_x = sorted_x[:100]\n",
    "    return sorted_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regional-midnight",
   "metadata": {
    "_cell_guid": "bf86e4c9-0126-4234-ae8c-471990034bbc",
    "_uuid": "52c6af6a-fc2a-4854-a9b0-1cd831feb104",
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.683475Z",
     "iopub.status.busy": "2021-05-17T12:45:40.682693Z",
     "iopub.status.idle": "2021-05-17T12:45:40.686770Z",
     "shell.execute_reply": "2021-05-17T12:45:40.686137Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030597,
     "end_time": "2021-05-17T12:45:40.686906",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.656309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done?!\n"
     ]
    }
   ],
   "source": [
    "# Note: Need to find back the kaggle notebook from which I find how to do this! \n",
    "def print_wordcloud(text_data,stop_word=None,max_words=200):\n",
    "    plt.rcParams['figure.figsize']=(15,9)\n",
    "    plt.rcParams['font.size']=12                #10 \n",
    "    plt.rcParams['savefig.dpi']=100             #72 \n",
    "    plt.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "    if(stop_word == None):\n",
    "        print(\"Is none\")\n",
    "        stopwords = set(STOPWORDS)\n",
    "    else:\n",
    "        print(\"Custom stop word sent to function\")\n",
    "        stopwords = stop_word\n",
    "        \n",
    "    wordcloud = WordCloud(\n",
    "                              background_color='white',\n",
    "                              stopwords=stopwords,\n",
    "                              max_words=max_words,\n",
    "                              max_font_size=40, \n",
    "                              random_state=42\n",
    "                             ).generate(str(text_data))\n",
    "\n",
    "    print(wordcloud)\n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    fig.savefig(\"word1.png\", dpi=900)\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "print(\"all done?!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preceding-notebook",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T12:45:40.743515Z",
     "iopub.status.busy": "2021-05-17T12:45:40.742742Z",
     "iopub.status.idle": "2021-05-17T12:45:40.745980Z",
     "shell.execute_reply": "2021-05-17T12:45:40.745467Z"
    },
    "papermill": {
     "duration": 0.041404,
     "end_time": "2021-05-17T12:45:40.746127",
     "exception": false,
     "start_time": "2021-05-17T12:45:40.704723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ed5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "#     print(arrays)\n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=21\n",
    "                ,svd_solver='full').fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "# cores = multiprocessing.cpu_count()\n",
    "\n",
    "# from gensim.models.phrases import Phrases, Phraser\n",
    "# sent = [row.split() for row in data_kenjee_corr[data_kenjee_corr['is_question' >-1]]['clean_comment']]\n",
    "# phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "# bigram = Phraser(phrases)\n",
    "\n",
    "# sentences = bigram[sent]\n",
    "\n",
    "# w2v_model = Word2Vec(min_count=20,\n",
    "#                      window=2,\n",
    "#                      vector_size=300,\n",
    "#                      sample=6e-5, \n",
    "#                      alpha=0.03, \n",
    "#                      min_alpha=0.0007, \n",
    "#                      negative=20,\n",
    "#                      workers=cores-1)\n",
    "# w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.865601,
   "end_time": "2021-05-17T12:45:42.236045",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-17T12:45:13.370444",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
